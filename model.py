# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lEHXVwUaQWYp4OjAeCsdd5zJV-aBqSbb
"""

# Note: Remove ! commands if running outside Colab
# !pip install ultralytics opencv-python matplotlib
import cv2
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from ultralytics import YOLO

# Colab-specific import - comment out if running outside Colab
try:
    from google.colab.patches import cv2_imshow
except ImportError:
    pass  # Running outside Colab

model = YOLO("yolo11l.pt")  # or yolo11m.pt / yolo8l.pt
conf_threshold = 0.3

# Note: Remove ! commands if running outside Colab
# !wget -O image.jpg "https://static0.moviewebimages.com/wordpress/wp-content/uploads/article/YFQdcXrsu64BJMMhEl6k2WPjorzYG4.jpg"

from PIL import Image
import matplotlib.pyplot as plt
import numpy as np

image_path = "YFQdcXrsu64BJMMhEl6k2WPjorzYG4.jpg"
# 1. Open the image file
image_pil = Image.open(image_path)   # change path if needed

# 2. Display it
plt.imshow(image_pil)
plt.axis("off")   # hide axes
plt.show()

# Convert PIL to OpenCV BGR format for cropping
image_cv = cv2.cvtColor(np.array(image_pil), cv2.COLOR_RGB2BGR)
h_img, w_img = image_cv.shape[:2]

results = model(
    image_path,
    conf=0.1,    # low confidence → more boxes
    iou=0.5,
    verbose=False,
)

from PIL import Image
import cv2

pil_crops = []   # list of PIL Images
expand_ratio = 0.1  # Expand bounding boxes by 10% on each side

for r in results:
    for box in r.boxes:
        x1, y1, x2, y2 = map(int, box.xyxy[0])

        w = x2 - x1
        h = y2 - y1

        dx = int(w * expand_ratio / 2)
        dy = int(h * expand_ratio / 2)

        x1_new = max(0, x1 - dx)
        y1_new = max(0, y1 - dy)
        x2_new = min(w_img, x2 + dx)
        y2_new = min(h_img, y2 + dy)

        # Crop using OpenCV (BGR)
        crop_bgr = image_cv[y1_new:y2_new, x1_new:x2_new]

        # Convert BGR → RGB
        crop_rgb = cv2.cvtColor(crop_bgr, cv2.COLOR_BGR2RGB)

        # Convert NumPy → PIL
        pil_img = Image.fromarray(crop_rgb)

        pil_crops.append(pil_img)

import matplotlib.pyplot as plt
import math

# Display initial crops grid (before VLM analysis)
num = len(pil_crops)
cols = 4
rows = math.ceil(num / cols)

plt.figure(figsize=(cols * 4, rows * 4))

for i, img in enumerate(pil_crops):
    plt.subplot(rows, cols, i + 1)
    plt.imshow(img)
    plt.axis("off")
    plt.title(f"Crop {i}")

plt.tight_layout()
plt.show()

# Note: Remove ! commands if running outside Colab
# !pip install qwen-vl-utils  # Upgrade Qwen-VL utilities
# ── Standard library ────────────────────────────────────────────
import os          # File‑system helpers (paths, env vars, etc.)
import random      # Lightweight randomness (e.g. sample prompts)
import textwrap    # Nicely format long strings for display
import io          # In‑memory byte streams (e.g. image buffers)
import requests    # Simple HTTP requests for downloading assets

# ── Numerical computing ─────────────────────────────────────────
import numpy as np  # Core array maths (fast, vectorised operations)

# ── Deep‑learning stack ─────────────────────────────────────────
import torch  # Tensor library + GPU acceleration
from transformers import (
    Qwen2_5_VLForConditionalGeneration,  # Multimodal LLM (image+text)
    AutoProcessor,                       # Paired tokenizer/feature‑extractor
)

# ── Imaging & visualisation ─────────────────────────────────────
from PIL import Image                    # Pillow: load/save/manipulate images
import matplotlib.pyplot as plt          # Quick plots in notebooks
import matplotlib.patches as patches     # Bounding‑box overlays, etc.

# ── Project‑specific helpers ────────────────────────────────────
from qwen_vl_utils import process_vision_info  # Post‑process Qwen outputs

# ── Notebook conveniences ──────────────────────────────────────
try:
    import IPython.display as ipd             # Inline display (images, audio, HTML)
except ImportError:
    pass  # Running outside IPython/Colab

# ── VLM Analysis Function ──────────────────────────────────────
def analyze_crop_with_vlm(crop_img, processor, vlm_model, device, prompt_text=None):
    """
    Analyze a single crop image with the VLM.
    
    Args:
        crop_img: PIL Image to analyze
        processor: AutoProcessor instance
        vlm_model: Qwen2_5_VLForConditionalGeneration model
        device: Device string ("cuda" or "cpu")
        prompt_text: Optional custom prompt. If None, uses default weapon analysis prompt.
    
    Returns:
        str: Analysis text/caption
    """
    if prompt_text is None:
        prompt_text = "Describe the weapon detected in the photo. Be very specific, type of weapon (eg handgun), brand if possible and if unsure don't say a brand. Describe the intent of the user with the gun, are they actively going to use it or are they passive. If unsure say that you are unsure unless certain."
    
    msgs = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": crop_img},
                {"type": "text", "text": prompt_text}
            ],
        }
    ]
    
    text_prompt = processor.apply_chat_template(
        msgs,
        tokenize=False,
        add_generation_prompt=True
    )
    
    image_inputs, video_inputs = process_vision_info(msgs)
    
    # ── Pack text + vision into model-ready tensors ──────────────────────────────
    inputs = processor(
        text=[text_prompt],
        images=image_inputs,
        videos=video_inputs,
        padding=True,
        return_tensors="pt",
    ).to(device)
    
    # ── Run inference (no gradients, pure generation) ───────────────────────────
    with torch.no_grad():
        generated_ids = vlm_model.generate(
            **inputs,
            max_new_tokens=64
        )
    
    # Extract the newly generated tokens (skip the prompt length)
    caption = processor.batch_decode(
        generated_ids[:, inputs.input_ids.shape[-1]:],
        skip_special_tokens=True
    )[0]
    
    return caption

# ── Load VLM Model Once ──────────────────────────────────────
print("Loading VLM model...")
device = "cuda" if torch.cuda.is_available() else "cpu"
model_id = "Qwen/Qwen2.5-VL-3B-Instruct"

vlm_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    model_id,
    torch_dtype="auto",     # automatically uses FP16 on GPU, FP32 on CPU
    device_map="auto"       # dispatches layers to the available device(s)
)
processor = AutoProcessor.from_pretrained(model_id)

print(f"Model loaded on: {vlm_model.device}")

# ── Analyze All Crops ──────────────────────────────────────
print(f"\nAnalyzing {len(pil_crops)} detected boxes with VLM...")
analysis_results = []  # List of (crop_image, analysis_text) tuples

for i, crop in enumerate(pil_crops):
    print(f"Processing crop {i+1}/{len(pil_crops)}...")
    analysis = analyze_crop_with_vlm(crop, processor, vlm_model, device)
    analysis_results.append((crop, analysis))
    print(f"Crop {i+1} analysis: {analysis[:100]}...")  # Print first 100 chars

# ── Display Grid with Analysis Results ──────────────────────────────────────
print("\nDisplaying results grid...")
num_results = len(analysis_results)
cols = 4
rows = math.ceil(num_results / cols)

# Create figure with enough space for images and text
plt.figure(figsize=(cols * 5, rows * 6))

for i, (img, analysis_text) in enumerate(analysis_results):
    plt.subplot(rows, cols, i + 1)
    plt.imshow(img)
    plt.axis("off")
    
    # Wrap the analysis text for display
    wrapped_text = textwrap.fill(analysis_text, width=40)
    plt.title(f"Crop {i}\n{wrapped_text}", fontsize=8, pad=5)

plt.tight_layout()
plt.show()

# ── Print All Analysis Results ──────────────────────────────────────
print("\n" + "="*80)
print("COMPLETE ANALYSIS RESULTS")
print("="*80)
for i, (img, analysis_text) in enumerate(analysis_results):
    print(f"\n--- Crop {i} Analysis ---")
    wrapped_caption = textwrap.fill(analysis_text, width=80)
    print(wrapped_caption)
    print()