{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ”« Weapon Detection & Analysis Pipeline\n",
        "\n",
        "**Vision-Language Model (VLM) powered weapon detection and analysis**\n",
        "\n",
        "This notebook provides a complete pipeline for:\n",
        "1. **Object Detection** - Using YOLOv11 to detect objects in images\n",
        "2. **Cropping** - Extracting detected regions with expanded bounding boxes\n",
        "3. **VLM Analysis** - Using Qwen2.5-VL to analyze each crop for weapon identification\n",
        "\n",
        "---\n",
        "**Designed for Google Colab** | GPU recommended for faster inference\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“¦ Step 1: Install Dependencies\n",
        "\n",
        "Run this cell to install all required packages. This only needs to be run once per session.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (Google Colab)\n",
        "!pip install -q ultralytics opencv-python matplotlib pillow\n",
        "!pip install -q transformers accelerate qwen-vl-utils\n",
        "!pip install -q torch torchvision torchaudio\n",
        "\n",
        "print(\"âœ… All dependencies installed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“š Step 2: Import Libraries\n",
        "\n",
        "Import all necessary libraries for the pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â”€â”€ Standard library â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import textwrap\n",
        "import io\n",
        "import requests\n",
        "\n",
        "# â”€â”€ Numerical computing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "import numpy as np\n",
        "\n",
        "# â”€â”€ Deep-learning stack â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "import torch\n",
        "from transformers import (\n",
        "    Qwen2_5_VLForConditionalGeneration,\n",
        "    AutoProcessor,\n",
        ")\n",
        "\n",
        "# â”€â”€ Imaging & visualization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# â”€â”€ Object detection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# â”€â”€ Project-specific helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "from qwen_vl_utils import process_vision_info\n",
        "\n",
        "# â”€â”€ Colab-specific â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "try:\n",
        "    from google.colab import files\n",
        "    from google.colab.patches import cv2_imshow\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "print(f\"ğŸ”§ Running in Google Colab: {IN_COLAB}\")\n",
        "print(f\"ğŸ–¥ï¸ PyTorch version: {torch.__version__}\")\n",
        "print(f\"ğŸ® CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âš™ï¸ Step 3: Configuration\n",
        "\n",
        "Set up configuration parameters for the pipeline. Modify these as needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# CONFIGURATION - Modify these parameters as needed\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Configuration class for the weapon detection pipeline.\"\"\"\n",
        "    \n",
        "    # â”€â”€ YOLO Detection Settings â”€â”€\n",
        "    YOLO_MODEL = \"yolo11l.pt\"           # Options: yolo11n.pt, yolo11s.pt, yolo11m.pt, yolo11l.pt\n",
        "    DETECTION_CONFIDENCE = 0.1          # Lower = more detections (0.0 to 1.0)\n",
        "    DETECTION_IOU = 0.5                 # IoU threshold for NMS\n",
        "    \n",
        "    # â”€â”€ Cropping Settings â”€â”€\n",
        "    EXPAND_RATIO = 0.1                  # Expand bounding boxes by this ratio (0.1 = 10%)\n",
        "    \n",
        "    # â”€â”€ VLM Settings â”€â”€\n",
        "    VLM_MODEL_ID = \"Qwen/Qwen2.5-VL-3B-Instruct\"  # Vision-Language Model\n",
        "    VLM_MAX_TOKENS = 64                 # Max tokens for VLM generation\n",
        "    \n",
        "    # â”€â”€ Visualization Settings â”€â”€\n",
        "    GRID_COLS = 4                       # Number of columns in result grid\n",
        "    FIG_SIZE_MULTIPLIER = 5             # Figure size multiplier\n",
        "    \n",
        "    # â”€â”€ Default Prompt â”€â”€\n",
        "    DEFAULT_PROMPT = (\n",
        "        \"Describe the weapon detected in the photo. Be very specific, \"\n",
        "        \"type of weapon (eg handgun), brand if possible and if unsure don't say a brand. \"\n",
        "        \"Describe the intent of the user with the gun, are they actively going to use it or are they passive. \"\n",
        "        \"If unsure say that you are unsure unless certain.\"\n",
        "    )\n",
        "\n",
        "config = Config()\n",
        "print(\"âœ… Configuration loaded!\")\n",
        "print(f\"   YOLO Model: {config.YOLO_MODEL}\")\n",
        "print(f\"   VLM Model: {config.VLM_MODEL_ID}\")\n",
        "print(f\"   Detection Confidence: {config.DETECTION_CONFIDENCE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ› ï¸ Step 4: Utility Functions\n",
        "\n",
        "Define reusable utility functions for image loading, visualization, and analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# UTILITY FUNCTIONS\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def load_image(image_source):\n",
        "    \"\"\"\n",
        "    Load an image from various sources (file path, URL, or PIL Image).\n",
        "    \n",
        "    Args:\n",
        "        image_source: str (file path or URL) or PIL.Image\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (PIL Image, OpenCV BGR image, height, width)\n",
        "    \"\"\"\n",
        "    if isinstance(image_source, Image.Image):\n",
        "        image_pil = image_source\n",
        "    elif image_source.startswith(('http://', 'https://')):\n",
        "        # Load from URL\n",
        "        response = requests.get(image_source)\n",
        "        image_pil = Image.open(io.BytesIO(response.content))\n",
        "        print(f\"ğŸ“¥ Downloaded image from URL\")\n",
        "    else:\n",
        "        # Load from file path\n",
        "        image_pil = Image.open(image_source)\n",
        "        print(f\"ğŸ“‚ Loaded image from: {image_source}\")\n",
        "    \n",
        "    # Convert to RGB if needed\n",
        "    if image_pil.mode != 'RGB':\n",
        "        image_pil = image_pil.convert('RGB')\n",
        "    \n",
        "    # Convert PIL to OpenCV BGR format\n",
        "    image_cv = cv2.cvtColor(np.array(image_pil), cv2.COLOR_RGB2BGR)\n",
        "    h_img, w_img = image_cv.shape[:2]\n",
        "    \n",
        "    print(f\"   Image size: {w_img} x {h_img} pixels\")\n",
        "    \n",
        "    return image_pil, image_cv, h_img, w_img\n",
        "\n",
        "\n",
        "def display_image(image, title=\"Image\", figsize=(10, 10)):\n",
        "    \"\"\"Display a PIL image with matplotlib.\"\"\"\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(title, fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def display_image_grid(images, titles=None, cols=4, figsize_multiplier=4):\n",
        "    \"\"\"\n",
        "    Display multiple images in a grid.\n",
        "    \n",
        "    Args:\n",
        "        images: List of PIL Images\n",
        "        titles: Optional list of titles for each image\n",
        "        cols: Number of columns in the grid\n",
        "        figsize_multiplier: Size multiplier for the figure\n",
        "    \"\"\"\n",
        "    num = len(images)\n",
        "    rows = math.ceil(num / cols)\n",
        "    \n",
        "    plt.figure(figsize=(cols * figsize_multiplier, rows * figsize_multiplier))\n",
        "    \n",
        "    for i, img in enumerate(images):\n",
        "        plt.subplot(rows, cols, i + 1)\n",
        "        plt.imshow(img)\n",
        "        plt.axis(\"off\")\n",
        "        if titles and i < len(titles):\n",
        "            plt.title(titles[i], fontsize=10)\n",
        "        else:\n",
        "            plt.title(f\"Image {i}\", fontsize=10)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def display_detections(image_pil, results, title=\"YOLO Detections\"):\n",
        "    \"\"\"\n",
        "    Visualize YOLO detection results on the original image.\n",
        "    \n",
        "    Args:\n",
        "        image_pil: Original PIL Image\n",
        "        results: YOLO results object\n",
        "        title: Title for the plot\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(1, figsize=(12, 12))\n",
        "    ax.imshow(image_pil)\n",
        "    \n",
        "    colors = plt.cm.tab10.colors\n",
        "    \n",
        "    for r in results:\n",
        "        for idx, box in enumerate(r.boxes):\n",
        "            x1, y1, x2, y2 = map(float, box.xyxy[0])\n",
        "            conf = float(box.conf[0])\n",
        "            cls = int(box.cls[0])\n",
        "            \n",
        "            color = colors[idx % len(colors)]\n",
        "            \n",
        "            rect = patches.Rectangle(\n",
        "                (x1, y1), x2 - x1, y2 - y1,\n",
        "                linewidth=2,\n",
        "                edgecolor=color,\n",
        "                facecolor='none'\n",
        "            )\n",
        "            ax.add_patch(rect)\n",
        "            \n",
        "            # Add label\n",
        "            label = f\"#{idx} conf:{conf:.2f}\"\n",
        "            ax.text(x1, y1 - 5, label, fontsize=9, color='white',\n",
        "                   bbox=dict(boxstyle='round,pad=0.3', facecolor=color, alpha=0.8))\n",
        "    \n",
        "    ax.axis(\"off\")\n",
        "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"ğŸ“Š Detected {len(r.boxes)} objects\")\n",
        "\n",
        "print(\"âœ… Utility functions defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# CROPPING FUNCTIONS\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def extract_crops(image_cv, results, expand_ratio=0.1):\n",
        "    \"\"\"\n",
        "    Extract cropped regions from YOLO detection results.\n",
        "    \n",
        "    Args:\n",
        "        image_cv: OpenCV BGR image\n",
        "        results: YOLO results object\n",
        "        expand_ratio: How much to expand bounding boxes (0.1 = 10%)\n",
        "    \n",
        "    Returns:\n",
        "        list: List of PIL Images (cropped regions)\n",
        "    \"\"\"\n",
        "    h_img, w_img = image_cv.shape[:2]\n",
        "    pil_crops = []\n",
        "    crop_info = []  # Store metadata about each crop\n",
        "    \n",
        "    for r in results:\n",
        "        for idx, box in enumerate(r.boxes):\n",
        "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "            conf = float(box.conf[0])\n",
        "            \n",
        "            w = x2 - x1\n",
        "            h = y2 - y1\n",
        "            \n",
        "            # Expand bounding box\n",
        "            dx = int(w * expand_ratio / 2)\n",
        "            dy = int(h * expand_ratio / 2)\n",
        "            \n",
        "            x1_new = max(0, x1 - dx)\n",
        "            y1_new = max(0, y1 - dy)\n",
        "            x2_new = min(w_img, x2 + dx)\n",
        "            y2_new = min(h_img, y2 + dy)\n",
        "            \n",
        "            # Crop using OpenCV (BGR)\n",
        "            crop_bgr = image_cv[y1_new:y2_new, x1_new:x2_new]\n",
        "            \n",
        "            # Convert BGR â†’ RGB â†’ PIL\n",
        "            crop_rgb = cv2.cvtColor(crop_bgr, cv2.COLOR_BGR2RGB)\n",
        "            pil_img = Image.fromarray(crop_rgb)\n",
        "            \n",
        "            pil_crops.append(pil_img)\n",
        "            crop_info.append({\n",
        "                'index': idx,\n",
        "                'confidence': conf,\n",
        "                'bbox_original': (x1, y1, x2, y2),\n",
        "                'bbox_expanded': (x1_new, y1_new, x2_new, y2_new),\n",
        "                'size': (x2_new - x1_new, y2_new - y1_new)\n",
        "            })\n",
        "    \n",
        "    print(f\"âœ‚ï¸ Extracted {len(pil_crops)} crops from detections\")\n",
        "    return pil_crops, crop_info\n",
        "\n",
        "print(\"âœ… Cropping functions defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ§  Step 5: VLM Analysis Functions\n",
        "\n",
        "Define the Vision-Language Model analysis functions for weapon detection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# VLM ANALYSIS FUNCTIONS\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def analyze_crop_with_vlm(crop_img, processor, vlm_model, device, prompt_text=None, max_tokens=64):\n",
        "    \"\"\"\n",
        "    Analyze a single crop image with the Vision-Language Model.\n",
        "    \n",
        "    Args:\n",
        "        crop_img: PIL Image to analyze\n",
        "        processor: AutoProcessor instance\n",
        "        vlm_model: Qwen2_5_VLForConditionalGeneration model\n",
        "        device: Device string (\"cuda\" or \"cpu\")\n",
        "        prompt_text: Optional custom prompt. If None, uses default weapon analysis prompt.\n",
        "        max_tokens: Maximum tokens for generation\n",
        "    \n",
        "    Returns:\n",
        "        str: Analysis text/caption\n",
        "    \"\"\"\n",
        "    if prompt_text is None:\n",
        "        prompt_text = config.DEFAULT_PROMPT\n",
        "    \n",
        "    msgs = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": crop_img},\n",
        "                {\"type\": \"text\", \"text\": prompt_text}\n",
        "            ],\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    text_prompt = processor.apply_chat_template(\n",
        "        msgs,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    \n",
        "    image_inputs, video_inputs = process_vision_info(msgs)\n",
        "    \n",
        "    # Pack text + vision into model-ready tensors\n",
        "    inputs = processor(\n",
        "        text=[text_prompt],\n",
        "        images=image_inputs,\n",
        "        videos=video_inputs,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(device)\n",
        "    \n",
        "    # Run inference (no gradients, pure generation)\n",
        "    with torch.no_grad():\n",
        "        generated_ids = vlm_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_tokens\n",
        "        )\n",
        "    \n",
        "    # Extract the newly generated tokens (skip the prompt length)\n",
        "    caption = processor.batch_decode(\n",
        "        generated_ids[:, inputs.input_ids.shape[-1]:],\n",
        "        skip_special_tokens=True\n",
        "    )[0]\n",
        "    \n",
        "    return caption\n",
        "\n",
        "\n",
        "def analyze_all_crops(pil_crops, processor, vlm_model, device, prompt_text=None, show_progress=True):\n",
        "    \"\"\"\n",
        "    Analyze all crops with the VLM.\n",
        "    \n",
        "    Args:\n",
        "        pil_crops: List of PIL Images\n",
        "        processor: AutoProcessor instance\n",
        "        vlm_model: Vision-Language Model\n",
        "        device: Device string\n",
        "        prompt_text: Optional custom prompt\n",
        "        show_progress: Whether to show progress updates\n",
        "    \n",
        "    Returns:\n",
        "        list: List of (crop_image, analysis_text) tuples\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    total = len(pil_crops)\n",
        "    \n",
        "    if show_progress:\n",
        "        print(f\"\\nğŸ” Analyzing {total} crops with VLM...\")\n",
        "        print(\"â”€\" * 50)\n",
        "    \n",
        "    for i, crop in enumerate(pil_crops):\n",
        "        if show_progress:\n",
        "            print(f\"  [{i+1}/{total}] Processing crop {i}...\", end=\" \")\n",
        "        \n",
        "        analysis = analyze_crop_with_vlm(\n",
        "            crop, processor, vlm_model, device, \n",
        "            prompt_text=prompt_text,\n",
        "            max_tokens=config.VLM_MAX_TOKENS\n",
        "        )\n",
        "        results.append((crop, analysis))\n",
        "        \n",
        "        if show_progress:\n",
        "            # Show preview of analysis\n",
        "            preview = analysis[:60] + \"...\" if len(analysis) > 60 else analysis\n",
        "            print(f\"âœ“\")\n",
        "            print(f\"      â†’ {preview}\")\n",
        "    \n",
        "    if show_progress:\n",
        "        print(\"â”€\" * 50)\n",
        "        print(f\"âœ… Completed analysis of {total} crops!\\n\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"âœ… VLM analysis functions defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# RESULTS DISPLAY FUNCTIONS\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "def display_analysis_results(analysis_results, cols=4, figsize_multiplier=5):\n",
        "    \"\"\"\n",
        "    Display analysis results in a grid with images and text.\n",
        "    \n",
        "    Args:\n",
        "        analysis_results: List of (crop_image, analysis_text) tuples\n",
        "        cols: Number of columns in the grid\n",
        "        figsize_multiplier: Size multiplier for the figure\n",
        "    \"\"\"\n",
        "    num_results = len(analysis_results)\n",
        "    rows = math.ceil(num_results / cols)\n",
        "    \n",
        "    # Create figure with enough space for images and text\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(cols * figsize_multiplier, rows * (figsize_multiplier + 1)))\n",
        "    \n",
        "    # Flatten axes for easy iteration\n",
        "    if rows == 1 and cols == 1:\n",
        "        axes = [[axes]]\n",
        "    elif rows == 1:\n",
        "        axes = [axes]\n",
        "    elif cols == 1:\n",
        "        axes = [[ax] for ax in axes]\n",
        "    \n",
        "    for i, (img, analysis_text) in enumerate(analysis_results):\n",
        "        row = i // cols\n",
        "        col = i % cols\n",
        "        ax = axes[row][col] if rows > 1 else axes[0][col] if cols > 1 else axes[0][0]\n",
        "        \n",
        "        ax.imshow(img)\n",
        "        ax.axis(\"off\")\n",
        "        \n",
        "        # Wrap the analysis text for display\n",
        "        wrapped_text = textwrap.fill(analysis_text, width=35)\n",
        "        ax.set_title(f\"Crop {i}\\n{wrapped_text}\", fontsize=8, pad=5, wrap=True)\n",
        "    \n",
        "    # Hide empty subplots\n",
        "    for i in range(num_results, rows * cols):\n",
        "        row = i // cols\n",
        "        col = i % cols\n",
        "        ax = axes[row][col] if rows > 1 else axes[0][col] if cols > 1 else axes[0][0]\n",
        "        ax.axis(\"off\")\n",
        "        ax.set_visible(False)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def print_full_analysis(analysis_results):\n",
        "    \"\"\"\n",
        "    Print complete analysis results in a formatted way.\n",
        "    \n",
        "    Args:\n",
        "        analysis_results: List of (crop_image, analysis_text) tuples\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"â•\" * 80)\n",
        "    print(\"ğŸ“‹ COMPLETE ANALYSIS RESULTS\")\n",
        "    print(\"â•\" * 80)\n",
        "    \n",
        "    for i, (img, analysis_text) in enumerate(analysis_results):\n",
        "        print(f\"\\nâ”Œâ”€ Crop {i} {'â”€' * 68}\")\n",
        "        wrapped_caption = textwrap.fill(analysis_text, width=76, initial_indent=\"â”‚ \", subsequent_indent=\"â”‚ \")\n",
        "        print(wrapped_caption)\n",
        "        print(f\"â””{'â”€' * 77}\")\n",
        "    \n",
        "    print(\"\\n\" + \"â•\" * 80)\n",
        "    print(f\"Total crops analyzed: {len(analysis_results)}\")\n",
        "    print(\"â•\" * 80)\n",
        "\n",
        "print(\"âœ… Results display functions defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”„ Step 6: Complete Pipeline Class\n",
        "\n",
        "A unified class that encapsulates the entire detection and analysis pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# WEAPON DETECTION PIPELINE CLASS\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "class WeaponDetectionPipeline:\n",
        "    \"\"\"\n",
        "    Complete pipeline for weapon detection and analysis.\n",
        "    \n",
        "    Usage:\n",
        "        pipeline = WeaponDetectionPipeline()\n",
        "        pipeline.load_models()\n",
        "        results = pipeline.analyze_image(\"path/to/image.jpg\")\n",
        "        pipeline.display_results(results)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, config=None):\n",
        "        \"\"\"Initialize the pipeline with optional custom config.\"\"\"\n",
        "        self.config = config or Config()\n",
        "        self.yolo_model = None\n",
        "        self.vlm_model = None\n",
        "        self.processor = None\n",
        "        self.device = None\n",
        "        self.models_loaded = False\n",
        "    \n",
        "    def load_models(self):\n",
        "        \"\"\"Load both YOLO and VLM models.\"\"\"\n",
        "        print(\"ğŸš€ Loading models...\")\n",
        "        print(\"â”€\" * 50)\n",
        "        \n",
        "        # Load YOLO model\n",
        "        print(f\"   Loading YOLO: {self.config.YOLO_MODEL}...\")\n",
        "        self.yolo_model = YOLO(self.config.YOLO_MODEL)\n",
        "        print(\"   âœ“ YOLO loaded\")\n",
        "        \n",
        "        # Set device\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        print(f\"   Device: {self.device}\")\n",
        "        \n",
        "        # Load VLM model\n",
        "        print(f\"   Loading VLM: {self.config.VLM_MODEL_ID}...\")\n",
        "        self.vlm_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "            self.config.VLM_MODEL_ID,\n",
        "            torch_dtype=\"auto\",\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        self.processor = AutoProcessor.from_pretrained(self.config.VLM_MODEL_ID)\n",
        "        print(\"   âœ“ VLM loaded\")\n",
        "        \n",
        "        self.models_loaded = True\n",
        "        print(\"â”€\" * 50)\n",
        "        print(\"âœ… All models loaded successfully!\\n\")\n",
        "    \n",
        "    def detect_objects(self, image_path, visualize=True):\n",
        "        \"\"\"\n",
        "        Run YOLO detection on an image.\n",
        "        \n",
        "        Args:\n",
        "            image_path: Path to image or URL\n",
        "            visualize: Whether to display detection visualization\n",
        "        \n",
        "        Returns:\n",
        "            tuple: (image_pil, image_cv, results, h_img, w_img)\n",
        "        \"\"\"\n",
        "        if not self.models_loaded:\n",
        "            raise RuntimeError(\"Models not loaded. Call load_models() first.\")\n",
        "        \n",
        "        # Load image\n",
        "        image_pil, image_cv, h_img, w_img = load_image(image_path)\n",
        "        \n",
        "        if visualize:\n",
        "            display_image(image_pil, title=\"Original Image\", figsize=(10, 10))\n",
        "        \n",
        "        # Run YOLO detection\n",
        "        print(\"\\nğŸ” Running YOLO detection...\")\n",
        "        results = self.yolo_model(\n",
        "            image_path if isinstance(image_path, str) else np.array(image_pil),\n",
        "            conf=self.config.DETECTION_CONFIDENCE,\n",
        "            iou=self.config.DETECTION_IOU,\n",
        "            verbose=False,\n",
        "        )\n",
        "        \n",
        "        if visualize:\n",
        "            display_detections(image_pil, results, title=\"YOLO Detection Results\")\n",
        "        \n",
        "        return image_pil, image_cv, results, h_img, w_img\n",
        "    \n",
        "    def analyze_image(self, image_path, prompt=None, visualize_steps=True):\n",
        "        \"\"\"\n",
        "        Complete analysis pipeline: detect â†’ crop â†’ analyze.\n",
        "        \n",
        "        Args:\n",
        "            image_path: Path to image or URL\n",
        "            prompt: Optional custom prompt for VLM\n",
        "            visualize_steps: Whether to show intermediate visualizations\n",
        "        \n",
        "        Returns:\n",
        "            dict: Complete analysis results\n",
        "        \"\"\"\n",
        "        if not self.models_loaded:\n",
        "            raise RuntimeError(\"Models not loaded. Call load_models() first.\")\n",
        "        \n",
        "        print(\"â•\" * 60)\n",
        "        print(\"ğŸ”« WEAPON DETECTION & ANALYSIS PIPELINE\")\n",
        "        print(\"â•\" * 60)\n",
        "        \n",
        "        # Step 1: Detect objects\n",
        "        image_pil, image_cv, yolo_results, h_img, w_img = self.detect_objects(\n",
        "            image_path, visualize=visualize_steps\n",
        "        )\n",
        "        \n",
        "        # Step 2: Extract crops\n",
        "        print(\"\\nâœ‚ï¸ Extracting detected regions...\")\n",
        "        pil_crops, crop_info = extract_crops(\n",
        "            image_cv, yolo_results, \n",
        "            expand_ratio=self.config.EXPAND_RATIO\n",
        "        )\n",
        "        \n",
        "        if visualize_steps and pil_crops:\n",
        "            titles = [f\"Crop {i} ({info['confidence']:.2f})\" for i, info in enumerate(crop_info)]\n",
        "            display_image_grid(pil_crops, titles=titles, cols=self.config.GRID_COLS)\n",
        "        \n",
        "        # Step 3: Analyze with VLM\n",
        "        if pil_crops:\n",
        "            analysis_results = analyze_all_crops(\n",
        "                pil_crops, self.processor, self.vlm_model, self.device,\n",
        "                prompt_text=prompt, show_progress=True\n",
        "            )\n",
        "        else:\n",
        "            print(\"âš ï¸ No objects detected to analyze.\")\n",
        "            analysis_results = []\n",
        "        \n",
        "        # Compile results\n",
        "        results = {\n",
        "            'image_pil': image_pil,\n",
        "            'yolo_results': yolo_results,\n",
        "            'crops': pil_crops,\n",
        "            'crop_info': crop_info,\n",
        "            'analysis_results': analysis_results,\n",
        "            'num_detections': len(pil_crops)\n",
        "        }\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def display_results(self, results, show_grid=True, print_text=True):\n",
        "        \"\"\"\n",
        "        Display analysis results.\n",
        "        \n",
        "        Args:\n",
        "            results: Results dict from analyze_image()\n",
        "            show_grid: Whether to show image grid\n",
        "            print_text: Whether to print full text analysis\n",
        "        \"\"\"\n",
        "        if not results['analysis_results']:\n",
        "            print(\"âš ï¸ No results to display.\")\n",
        "            return\n",
        "        \n",
        "        if show_grid:\n",
        "            display_analysis_results(\n",
        "                results['analysis_results'],\n",
        "                cols=self.config.GRID_COLS,\n",
        "                figsize_multiplier=self.config.FIG_SIZE_MULTIPLIER\n",
        "            )\n",
        "        \n",
        "        if print_text:\n",
        "            print_full_analysis(results['analysis_results'])\n",
        "\n",
        "print(\"âœ… WeaponDetectionPipeline class defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¯ Step 7: Load Image\n",
        "\n",
        "Load your image for analysis. You can use a file path, URL, or upload from your computer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# OPTION A: Download sample image from URL\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "# Sample image URL (you can replace with your own)\n",
        "IMAGE_URL = \"https://static0.moviewebimages.com/wordpress/wp-content/uploads/article/YFQdcXrsu64BJMMhEl6k2WPjorzYG4.jpg\"\n",
        "\n",
        "# Download the image\n",
        "!wget -q -O sample_image.jpg \"{IMAGE_URL}\"\n",
        "IMAGE_PATH = \"sample_image.jpg\"\n",
        "\n",
        "print(f\"ğŸ“¥ Downloaded sample image to: {IMAGE_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# OPTION B: Upload your own image (Colab only)\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "# Uncomment the following lines to upload your own image in Google Colab:\n",
        "\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# IMAGE_PATH = list(uploaded.keys())[0]\n",
        "# print(f\"ğŸ“¤ Uploaded: {IMAGE_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# PREVIEW THE IMAGE\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "# Load and display the image\n",
        "image_pil, image_cv, h_img, w_img = load_image(IMAGE_PATH)\n",
        "display_image(image_pil, title=f\"Input Image ({w_img}x{h_img})\", figsize=(12, 12))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸš€ Step 8: Initialize and Load Models\n",
        "\n",
        "Initialize the pipeline and load the YOLO and VLM models. This may take a few minutes on first run.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# INITIALIZE PIPELINE AND LOAD MODELS\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "# Create pipeline instance\n",
        "pipeline = WeaponDetectionPipeline(config)\n",
        "\n",
        "# Load all models (YOLO + VLM)\n",
        "pipeline.load_models()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ” Step 9: Run Detection & Analysis\n",
        "\n",
        "Run the complete pipeline on your image. This will:\n",
        "1. Detect objects using YOLO\n",
        "2. Crop detected regions\n",
        "3. Analyze each crop with the VLM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# RUN COMPLETE ANALYSIS\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "# Run the full pipeline with visualization at each step\n",
        "results = pipeline.analyze_image(IMAGE_PATH, visualize_steps=True)\n",
        "\n",
        "print(f\"\\nğŸ“Š Summary: Detected and analyzed {results['num_detections']} objects\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“Š Step 10: Display Final Results\n",
        "\n",
        "View the complete analysis results with visualizations and detailed text output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# DISPLAY FINAL RESULTS\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "# Display results grid with analysis text\n",
        "pipeline.display_results(results, show_grid=True, print_text=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”§ Advanced: Analyze Individual Crops\n",
        "\n",
        "You can also analyze individual crops with custom prompts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# ANALYZE SINGLE CROP WITH CUSTOM PROMPT\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "# Select a specific crop to analyze (change index as needed)\n",
        "if results['crops']:\n",
        "    crop_index = 0  # Change this to analyze different crops\n",
        "    selected_crop = results['crops'][crop_index]\n",
        "    \n",
        "    # Display the selected crop\n",
        "    display_image(selected_crop, title=f\"Selected Crop {crop_index}\", figsize=(6, 6))\n",
        "    \n",
        "    # Custom prompt example\n",
        "    custom_prompt = \"Describe this image in detail. What objects do you see? What is the context?\"\n",
        "    \n",
        "    # Analyze with custom prompt\n",
        "    custom_analysis = analyze_crop_with_vlm(\n",
        "        selected_crop,\n",
        "        pipeline.processor,\n",
        "        pipeline.vlm_model,\n",
        "        pipeline.device,\n",
        "        prompt_text=custom_prompt,\n",
        "        max_tokens=128  # More tokens for detailed response\n",
        "    )\n",
        "    \n",
        "    print(\"ğŸ“ Custom Analysis Result:\")\n",
        "    print(\"â”€\" * 50)\n",
        "    print(textwrap.fill(custom_analysis, width=70))\n",
        "else:\n",
        "    print(\"âš ï¸ No crops available to analyze.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“ Alternative: Step-by-Step Manual Execution\n",
        "\n",
        "If you prefer more control, you can run each step manually instead of using the pipeline class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# MANUAL STEP-BY-STEP EXECUTION (Alternative to Pipeline)\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# Uncomment the code below to run steps manually\n",
        "\n",
        "\"\"\"\n",
        "# â”€â”€ Step 1: Load YOLO Model â”€â”€\n",
        "yolo_model = YOLO(\"yolo11l.pt\")\n",
        "\n",
        "# â”€â”€ Step 2: Load Image â”€â”€\n",
        "image_path = \"your_image.jpg\"\n",
        "image_pil, image_cv, h_img, w_img = load_image(image_path)\n",
        "display_image(image_pil, title=\"Original Image\")\n",
        "\n",
        "# â”€â”€ Step 3: Run YOLO Detection â”€â”€\n",
        "yolo_results = yolo_model(image_path, conf=0.1, iou=0.5, verbose=False)\n",
        "display_detections(image_pil, yolo_results)\n",
        "\n",
        "# â”€â”€ Step 4: Extract Crops â”€â”€\n",
        "pil_crops, crop_info = extract_crops(image_cv, yolo_results, expand_ratio=0.1)\n",
        "display_image_grid(pil_crops, cols=4)\n",
        "\n",
        "# â”€â”€ Step 5: Load VLM Model â”€â”€\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "vlm_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "    \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")\n",
        "\n",
        "# â”€â”€ Step 6: Analyze Crops â”€â”€\n",
        "analysis_results = analyze_all_crops(pil_crops, processor, vlm_model, device)\n",
        "\n",
        "# â”€â”€ Step 7: Display Results â”€â”€\n",
        "display_analysis_results(analysis_results)\n",
        "print_full_analysis(analysis_results)\n",
        "\"\"\"\n",
        "\n",
        "print(\"ğŸ’¡ Uncomment the code above to run the pipeline step-by-step manually.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ§¹ Cleanup (Optional)\n",
        "\n",
        "Free up GPU memory when done.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "# CLEANUP - Free GPU memory\n",
        "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "\n",
        "# Uncomment to free GPU memory when done:\n",
        "\n",
        "# del pipeline\n",
        "# del results\n",
        "# torch.cuda.empty_cache()\n",
        "# print(\"ğŸ§¹ GPU memory cleared!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
